{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multimodal Fusion Pipeline\n",
    "\n",
    "✅ Multimodal fusion enhances AI models by combining vision + text embeddings.\n",
    "\n",
    "✅ Self-attention refines features, and masking removes irrelevant data.\n",
    "\n",
    "✅ Pseudo-Patch Encoding ensures structured feature representation before classification.\n",
    "\n",
    "✅ EfficientNetV2 + LLM + CLIP + Self-Attention forms a powerful model pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps for Multimodal Fusion Model\n",
    "\n",
    "    1\tLoad dependencies\n",
    "    2\tImage Feature Extractor (EfficientNetV2)- 1280-dimensional image embedding\n",
    "    3\tText Feature Extractor (BERT)- 768-dimensional embedding\n",
    "    4\tFeature Projection Layers - 512D space\n",
    "    5\tFusion and Attention -->  projected image and text embeddings (→ 1024D)\n",
    "    6\tConvert attention vectors into Pseudo-Patch Encoder\n",
    "    7\tRegression Head\n",
    "    8\tTrain and optimize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Load Dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from transformers import AutoTokenizer, AutoModel, CLIPModel, CLIPProcessor\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Define Data Augmentation for Images\n",
    "transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Simplified Multimodal Regression Architecture using EfficientNetV2 + BERT + Self-Attention\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------\n",
    "# EfficientNetV2 Feature Extractor (Image)\n",
    "# ---------------------------\n",
    "class EfficientNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        efficientnet = models.efficientnet_v2_l(weights=\"IMAGENET1K_V1\")\n",
    "        self.feature_extractor = nn.Sequential(*list(efficientnet.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.feature_extractor(x).squeeze()\n",
    "\n",
    "# ---------------------------\n",
    "# Text Embedding using BERT\n",
    "# ---------------------------\n",
    "class TextEmbeddingLLM(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, text):\n",
    "        tokens = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        outputs = self.model(**tokens)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        return embeddings\n",
    "\n",
    "# ---------------------------\n",
    "# Self-Attention Layer\n",
    "# ---------------------------\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        attention = self.softmax(Q @ K.transpose(-2, -1) / (Q.size(-1) ** 0.5))\n",
    "        return attention @ V\n",
    "\n",
    "# ---------------------------\n",
    "# Pseudo-Patch Encoder\n",
    "# ---------------------------\n",
    "class PseudoPatchEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, patch_size=16):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(input_dim, input_dim // patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.projection(x)\n",
    "\n",
    "# ---------------------------\n",
    "# Multimodal Regressor with Attention and Patch Encoding\n",
    "# ---------------------------\n",
    "class MultimodalRegressor(nn.Module):\n",
    "    def __init__(self, image_dim=1280, text_dim=768, embed_dim=512):\n",
    "        super().__init__()\n",
    "        self.image_fc = nn.Linear(image_dim, embed_dim)\n",
    "        self.text_fc = nn.Linear(text_dim, embed_dim)\n",
    "        self.attention = SelfAttention(embed_dim)\n",
    "        self.pseudo_patch = PseudoPatchEncoder(embed_dim)\n",
    "        self.regressor = nn.Linear(embed_dim // 16, 1)\n",
    "\n",
    "    def forward(self, image_feat, text_feat):\n",
    "        img_embed = self.image_fc(image_feat)\n",
    "        txt_embed = self.text_fc(text_feat)\n",
    "        combined = torch.cat([img_embed, txt_embed], dim=-1)\n",
    "        attended = self.attention(combined.unsqueeze(0)).squeeze(0)\n",
    "        encoded = self.pseudo_patch(attended)\n",
    "        return self.regressor(encoded)\n",
    "\n",
    "# ---------------------------\n",
    "# Dummy Dataset (for Regression)\n",
    "# ---------------------------\n",
    "class DummyMultimodalDataset(Dataset):\n",
    "    def __init__(self, num_samples=100):\n",
    "        self.num_samples = num_samples\n",
    "        self.images = torch.randn(num_samples, 3, 224, 224)\n",
    "        self.texts = [\"This is a sample text.\"] * num_samples\n",
    "        self.kpi_targets = torch.randn(num_samples, 1) * 100\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"image\": self.images[idx],\n",
    "            \"text\": self.texts[idx],\n",
    "            \"kpi\": self.kpi_targets[idx]\n",
    "        }\n",
    "\n",
    "# ---------------------------\n",
    "# Training Function\n",
    "# ---------------------------\n",
    "def train_model(model, dataloader, optimizer, criterion, image_model, text_model):\n",
    "    model.train()\n",
    "    image_model.eval()\n",
    "    text_model.eval()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        image_feat = image_model(batch[\"image\"])\n",
    "        text_feat = text_model(batch[\"text\"])\n",
    "        output = model(image_feat, text_feat)\n",
    "        loss = criterion(output, batch[\"kpi\"])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    train_dataset = DummyMultimodalDataset(num_samples=100)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    image_model = EfficientNetFeatureExtractor()\n",
    "    text_model = TextEmbeddingLLM()\n",
    "    model = MultimodalRegressor()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.L1Loss()  # MAE for regression\n",
    "\n",
    "    for epoch in range(5):\n",
    "        train_loss = train_model(model, train_loader, optimizer, criterion, image_model, text_model)\n",
    "        print(f\"Epoch {epoch+1} - Train MAE: {train_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"best_multimodal_regressor.pt\")\n",
    "    print(\"Model saved as 'best_multimodal_regressor.pt'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda40_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
